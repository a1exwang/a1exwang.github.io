---
layout: post
title:  "负载均衡模型"
date:   2018-12-08 21:59:05 +0800
categories: load_balancer distributed_system
use_math: true
---
# 负载均衡的模型化讨论

## 场景
有N个服务端，保存相同数据，客户端按照固定间隔T，独立地向服务端发出请求，每个请求随机被分配到某一个
服务端处理，忽略处理请求的时间。

## 假设
1. 服务端数量为 $N$;
2. 在时间 $T$ 内，总共进行了 $k$ 次请求;
3. 客户端的每次请求独立;
4. 每个服务端性能参数完全相同;

## 计算
对于某一个服务器，记为 $S_0$，某一次请求，被分配到 $S_0$ 的概率为 $\frac{1}{N}$，假设进行了 $k$ 次，根据独立假设，每次的概率均为 $\frac{1}{N}$。设这  $k$ 次中被分配到 $S_0$ 的次数为随机变量 X，

$$X \sim B(k, \frac{1}{N})$$

当 k 趋于无穷大时，可近似的认为

$$ X \sim N(\frac{k}{N}, (\frac{k(1-\frac{1}{N})}{N})^2). $$

所以可以根据正态分布的 CDF 求出，$P(X > \mu k \frac{1}{N})$，也就是服务器
$S_0$ 的负载在 $k$ 次请求内大于 $\mu$ 倍期望值的概率。

记负载的期望值为 $L$，记正态分布$N(\mu, \sigma^2)$，的累计分布函数为 $cdf(\mu, \sigma, x)$
$$
L = \frac{k}{N}
$$

$$
P(X > \mu L) = 1 - cdf(L, L(1-\frac{1}{N})) \\
= 1 - \int_{-\infty}^{\mu L} \frac{1}{\sqrt{2 \pi} L(1-\frac{1}{N})} exp(- \frac{x-L}{L(1-\frac{1}{N})}^2) dx  \\
$$

考虑到该概率应该与 $L$ 无关，可以通过变量代换来消去 $L$。不妨设
$$
t = \frac{x - L}{L}.
$$

带入上式得出
$$
1 - \int_{-\infty}^{\mu - 1} \frac{1}{\sqrt{2\pi}(1-\frac{1}{N})} exp(-\frac{t}{1-\frac{1}{N}}^2)dt.
$$
我们发现，已经消去了 $L$, 为了将自变量也消去 $L$，设随机变量
$$
Y = \frac{X}{L}
$$

则
$$
P(Y > \mu) =
1 - \int_{-\infty}^{\mu - 1} \frac{1}{\sqrt{2\pi}(1-\frac{1}{N})} exp(-\frac{t}{1-\frac{1}{N}}^2)dt \\
= 1 - cdf(0, 1-\frac{1}{N}, \mu)
$$

## 上式的实际意义解释
上述各个参数的实际意义解释。

首先，$N$是实际的服务器数量，之所以令其为变量，而不是直接赋一个实际值可以可以便于之后根据其他限制反推出应该搭建的服务器数量。

期望 $L$ 的实际意义。在该场景中，我们往往会根据客户端总负载量（比如 IOPS，或者吞吐量）来计算服务端至少需要的性能参数。但是当客户端总负载量等于服务端最大性能参数（例如客户端总IOPS等于服务端能提供的总IOPS）时，我们往往会观测到客户端性能下降，而服务端有些负载很高有些负载很低，即负载不均衡的情况。而这个 $L=\frac{k}{N}$，也即根据这样的计算算出的单机最大性能指标。例如，我们预计客户端最大 IOPS 为 30000/s，那么我们部署 3 台能提供 10000/s 的机器，此时 $L = 10000$。当然这里也可以选择参数 $k=30000$ 作为计算的子变量，但是这样会导致算式复杂，所以这里选择了 $L$。

$P(Y > \mu)$ 的意义。其中 $\mu,Y$ 是在本题目假设下，服务器 $S_0$ 的实际负载和期望值的比值， $x = \mu L, Y = XL$ ，注意 $X,Y$ 为随机变量，$\mu$ 为普通变量。该值反映了服务器  $S_0$ 的过载比。而 $P(Y > \mu)$ 为服务器 $S_0$ 的负载大于过载比 $\mu$ 的概率。比如 $P(Y > 2) = 0.07$ 代表了 $S_0$ 负载超过 200% 的概率为 7%。

## 曲线实际意义
根据以上各个参数实际意义的解释，我们可以以 $\mu$ 为横坐标，$P(Y>\mu)$ 为纵坐标的画出一条曲线，再让 $N$ 变化，可以画出一组曲线。

如下图所示

![plot1.png](/assets/load_balancer_plot1.png)

该曲线代表了超载比和实际负载大于超载比的概率。在实际中，每台服务器的是有最大性能指标的，当超载比超过了实际的最大性能指标就会出现拥塞（比如单台服务器的最大 IOPS为 10000，期望 $L=2000$，则当超载比 $\mu > 5$ 时就会出现拥塞，导致客户端性能下降的情况）。这是我们不希望出现的，但是从概率上说，这种情况无法避免，只能尽量减少这种情况发生的概率。比如当我们知道单台服务端可承受的最大负载比为 $\mu=3$，我们希望一分钟出现一秒钟这样的情况，则可以令 $P(Y > \mu) = \frac{1}{60}$，从而求出 $N \approx 2.6$, 所以应该部署 3 台服务端。

类似的我们也可以固定 $N,P(Y > \mu)$，算出最大负载比 $\mu$。
上图中的水平线就是用这种方法，求出有 $N$ 台服务端时，应该选择的最大负载比。

## 上面推导应用于阿里云 SeaweedFS 集群
当前的阿里云 SeaweedFS 集群包含三台机器，每台机器有 10G 网卡(1GB/s)，两块对于 100k 文件有 8k IOPS 的 SSD，即总共 6块 SSD。（实际上主要的性能瓶颈在网络带宽和 SSD 的 IOPS，所以此处略去其他因素）我们不妨假设任意的 SSD 上都保存了客户端所需要的数据，也就是 replication 数量等于服务端数量，不会因为数据分布不均匀导致的负载不均匀。再假设期望的拥塞频率不超过每分钟拥塞一秒钟，以频率代替概率。设客户端使用 ROCS 训练模型，batch_size为32，8 卡训练，总共 a 台客户端机器，每台机器训练速度为每秒 r 次迭代。

分两种情况讨论：

1. 如果SSD IOPS 是瓶颈。对于这种情况，服务端数量 $N=6$。 可以算出最大负载比 $\mu \approx 2.90$.
$$
8 \times 32 \times r a \times \mu = 6 \times 8k, \mu = 2.90 \\
ra \approx 64.66 machines \cdot iters/s \\
\mbox{if}, r = 7, a = 9.23
$$
2. 如果网络带宽是瓶颈。对于这种情况，服务端数量 $N=3$。可以算出最大负载比 $\mu \approx 2.45$，所以可以反推出实际能承载的客户端迭代速度。

$$
8 \times 32 \times r a \times 100k \times \mu = 3 \times 1G, \mu = 2.45 \\
ra \approx 42.86 machines \cdot iters/s \\
\mbox{if}, r = 7, a = 6.12
$$

## 结论
当前阿里云上的 SeaweedFS 集群理论上最多能支撑大约 6 台 8 卡机器同时使用 shufflenet 训练 imagenet1k 数据集。
